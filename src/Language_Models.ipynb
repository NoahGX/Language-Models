{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Language Models**\n",
    "\n",
    "In this Jupyter Notebook, we will explore techniques for the development and implementation of a trigram language model, such as model training, evaluation, and testing to ensure accuracy and performance. This is a hands-on approach to understanding how basic statistical language models operate and how they can be applied to predict the probability of sequences of words in a language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:03:34.527385Z",
     "start_time": "2023-04-19T11:03:31.620082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%precision 4\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from cytoolz import concat, sliding_window\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:04:49.172418Z",
     "start_time": "2023-04-19T11:04:49.105020Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.autoconfig()\n",
    "\n",
    "    def init_test():\n",
    "        ipytest.clean()\n",
    "\n",
    "    def run_test():\n",
    "        ipytest.run()\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    def init_test():\n",
    "        pass\n",
    "\n",
    "    def run_test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **1. Trigram Language Model**\n",
    "\n",
    "We load some data and use it to train a simple trigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:04:53.484493Z",
     "start_time": "2023-04-19T11:04:53.016070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    return [line.lower().split() for line in gzip.open(filename)]\n",
    "\n",
    "\n",
    "sentences = read_corpus(\"../data/bnc_train.txt.gz\")\n",
    "sentences_train = sentences[:175000]\n",
    "sentences_test = sentences[175000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T12:29:25.233816Z",
     "start_time": "2023-04-19T12:29:25.202205Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TrigramLM:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"Normalize sentence and add filler tokens <s> and </s>\"\"\"\n",
    "        return [\"<s>\", \"<s>\"] + [w.lower() for w in sentence] + [\"</s>\",\"</s>\"]\n",
    "\n",
    "    def get_unigram_counts(self, train_corpus):\n",
    "        self.unigrams = Counter(concat(train_corpus))\n",
    "\n",
    "    def get_bigram_counts(self, train_corpus):\n",
    "        self.bigrams = Counter(sliding_window(2, concat(train_corpus)))\n",
    "\n",
    "    def get_trigram_counts(self, train_corpus):\n",
    "        self.trigrams = Counter(sliding_window(3, concat(train_corpus)))\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        \"\"\"Count bigram and unigram frequencies in the training corpus.\"\"\"\n",
    "        train_corpus = [self.preprocess(sentence) for sentence in train_corpus]\n",
    "        self.get_unigram_counts(train_corpus)\n",
    "        self.get_bigram_counts(train_corpus)\n",
    "        self.get_trigram_counts(train_corpus)\n",
    "        self.V = len(self.unigrams)\n",
    "\n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"Calculate the log_2 probability of a sentence given the model.\"\"\"\n",
    "        p = 0.0\n",
    "        try:\n",
    "            for (w1, w2, w3) in sliding_window(3, self.preprocess(sentence)):\n",
    "                p = (\n",
    "                        p\n",
    "                        + np.log2(self.trigrams[w1, w2, w3] + self.alpha)\n",
    "                        - np.log2(self.bigrams[w1, w2] + self.alpha * self.V)\n",
    "                )\n",
    "            return p\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T12:29:30.238219Z",
     "start_time": "2023-04-19T12:29:26.493339Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-104.2725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = TrigramLM(alpha=0.1)\n",
    "lm.train(sentences_train)\n",
    "lm.log_prob('This is a test'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **2. Perplexity**\n",
    "\n",
    "We will define a function that calculates the perplexity of a model on a corpus (= a list of sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, sentences):\n",
    "    total_log_prob = 0.0\n",
    "    N = 0  # Total number of words\n",
    "    for sentence in sentences:\n",
    "        total_log_prob += model.log_prob(sentence)\n",
    "        N += len(model.preprocess(sentence)) - 3\n",
    "    # Calculate average log probability per word and convert to perplexity\n",
    "    return 2 ** (-(total_log_prob / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15418.3348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = TrigramLM(alpha=0.1)\n",
    "lm.train(sentences_train)\n",
    "perplexity(lm, sentences_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:06:00.123838Z",
     "start_time": "2023-04-19T11:05:56.298965Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 7.08s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def my_trigram_lm():\n",
    "    lm = TrigramLM(alpha=0.1)\n",
    "    lm.train(sentences_train)\n",
    "    return lm\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"sentence,logprob\", [(sentences_train[0], -211.8753), (sentences_test[0], -86.1634)]\n",
    ")\n",
    "def test_trigram_logprob(my_trigram_lm, sentence, logprob):\n",
    "    assert my_trigram_lm.log_prob(sentence) == pytest.approx(logprob, rel=1e-3)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"sentences,perplex\",\n",
    "    [(sentences_train[:100], 3773.9770), (sentences_test[:100], 15418.3348)],\n",
    ")\n",
    "def test_trigram_perplexity(my_trigram_lm, sentences, perplex):\n",
    "    assert perplexity(my_trigram_lm, sentences) == pytest.approx(perplex, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **3. Smoothing**\n",
    "\n",
    "In the definition for `TrigramLM`, `alpha` is the smoothing parameter. In order to find out what the best value to use is, we will try building models with different values for `alpha` and then compute their perplexity on both `sentences_train[:500]` and `sentences_test[:500]`. For `alpha` values, we will try different powers of 10 (e.g., `[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T13:50:38.188254Z",
     "start_time": "2023-04-19T13:50:02.716618Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.00001, Train Perplexity: 1.8123, Test Perplexity: 6568.7423\n",
      "Alpha: 0.00010, Train Perplexity: 2.2011, Test Perplexity: 4095.1210\n",
      "Alpha: 0.00100, Train Perplexity: 5.7043, Test Perplexity: 2918.1654\n",
      "Alpha: 0.01000, Train Perplexity: 35.6928, Test Perplexity: 2620.1673\n",
      "Alpha: 0.10000, Train Perplexity: 294.2772, Test Perplexity: 2798.6211\n"
     ]
    }
   ],
   "source": [
    "# Define range of alpha values for testing\n",
    "alpha_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "train_perplexities = []\n",
    "test_perplexities = []\n",
    "\n",
    "# Iterate over alpha values to calculate perplexities\n",
    "for alpha in alpha_values:\n",
    "    lm = TrigramLM(alpha=alpha)\n",
    "    lm.train(sentences_train[:500])\n",
    "\n",
    "    # Calculate perplexity on training subset\n",
    "    train_perplexity = perplexity(lm, sentences_train[:500])\n",
    "    train_perplexities.append(train_perplexity)\n",
    "\n",
    "    # Calculate perplexity on testing subset\n",
    "    test_perplexity = perplexity(lm, sentences_test[:500])\n",
    "    test_perplexities.append(test_perplexity)\n",
    "\n",
    "# Output results\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    print(f\"Alpha: {alpha:.5f}, Train Perplexity: {train_perplexities[i]:.4f}, Test Perplexity: {test_perplexities[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we obtained from Problem #2 provide us with several patterns. These patterns tells us how the smoothing parameter `alpha` affects the perplexity on both the training and testing datasets for the Trigram Language Model.\n",
    "\n",
    "1. **Low `alpha` values** `(1e-5)` result in low training perplexity but high test perplexity. This pattern tells us that when we are dealing with very small `alpha` values, the model is most likely overfitting in regards to the training data. The model performs well on the training data because it closely captures the specific trigram frequencies within that dataset. However, it generalizes poorly in regards to the testing data, which leads to high perplexity scores when encountering new or unseen trigrams.\n",
    "\n",
    "2. **Medium `alpha` values** `(1e-4, 1e-3, 1e-2)` result in the test perplexity initially decreasing, reaching a minimum around `alpha=0.01`. This pattern tells us that when we introduce more smoothing to the model, it helps us to mitigate overfitting by allocating a small probability to any unseen trigrams. Thus, improving the model's overall ability to generalize in regards to new data. It is at this point that the optimal balance between underfitting and overfitting occurs, where the model still retains the ability to distinguish between common and rare trigrams effectively, but is robust enough to handle unseen data.\n",
    "\n",
    "3. **High `alpha` values** `(1e-1)` result in an increase of both training and testing perplexities. This pattern tells us that too much smoothing causes the language model to underfit the data. When the value of `alpha` becomes too large, the model begins to treat all trigrams the same, this results in the loss of its ability to differentiate trigrams based on their actual frequencies in the training data. Such over-smoothing considerably weakens the predictive power of the model, leading to overall poorer performance on both the training and testing datasets.\n",
    "\n",
    "Based on these observations, the best value of `alpha` seems to be `alpha=0.001`. This value not only minimizes the testing perplexity, but also indicates an optimal balance between giving enough probability to unseen trigrams and maintaining the model's ability to differentiate between trigrams based on their respective frequencies, thus avoiding zero probabilities. This increase in perplexity for values larger than `0.001` shows us the principle of diminishing returns, where additional smoothing begins to harm the model's performance instead of helping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## **4. Random Sampling**\n",
    "\n",
    "Now, we write a function that generates a random sentence by sampling from a trigram language model.\n",
    "\n",
    "Here's the basic approach we will take: Every sentence will start with the start symbols `<s> <s>`. The language model gives us the conditional probability of each possible word given that context:\n",
    "\n",
    "$$P(w_1|\\texttt{<s> <s>})=\\frac{C(\\texttt{<s> <s> } w_1)+\\alpha}{C(\\texttt{<s> <s>})+\\alpha V}$$\n",
    "\n",
    "We pick word $w_1$ at random by drawing from this distribution. Let's say the word we pick is `disgruntled`. Now, the probability of any word being the next word in the sentence is:\n",
    "\n",
    "$$P(w_2|\\texttt{<s> disgruntled})=\\frac{C(\\texttt{<s> disgruntled } w_2)+\\alpha}{C(\\texttt{<s> disgruntled})+\\alpha V}$$\n",
    "\n",
    "We pick word $w_2$ at random by drawing from this new distribution and keep going like this until we've picked 50 words or the next word is `</s>` (and the sentence is finished), whichever comes first.\n",
    "\n",
    "To do the random picking, we will use the function `multinomial` defined below. It takes a dictionary mapping words to probabilities and chooses one word at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'c': 249, 'a': 135, 'b': 116})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "def multinomial(probs):\n",
    "    X, p = zip(*probs.items())\n",
    "    return X[rng.multinomial(1, p).argmax()]\n",
    "\n",
    "P = {'a': 0.25, 'b': 0.25, 'c': 0.5}\n",
    "Counter(multinomial(P) for _ in range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(lm):\n",
    "    sentence = ['<s>', '<s>']  # Start with initial context\n",
    "    while len(sentence) < 52:  # Allow up to 50 words plus 2 starting tokens\n",
    "        context = (sentence[-2], sentence[-1])\n",
    "        possible_words = {}\n",
    "        \n",
    "        # Calculate conditional probability distribution for next word\n",
    "        for w3 in lm.unigrams:  # Consider all possible next words\n",
    "            trigram_C = lm.trigrams.get((context[0], context[1], w3), 0)\n",
    "            bigram_C = lm.bigrams.get((context[0], context[1]), 0)\n",
    "            possible_words[w3] = (trigram_C + lm.alpha) / (bigram_C + (lm.alpha * lm.V))\n",
    "        \n",
    "        # Sample next word\n",
    "        next_word = multinomial(possible_words)\n",
    "        if next_word == '</s>':  # Stop if end of sentence is reached\n",
    "            break\n",
    "        if isinstance(next_word, bytes):\n",
    "            next_word = next_word.decode('utf-8')\n",
    "        sentence.append(next_word)\n",
    "    \n",
    "    # Return generated sentence, excluding initial starting tokens\n",
    "    return ' '.join(sentence[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to observe the effect of alpha on the generated sentences, we can create language models with different alpha values and train them on the same dataset, allowing us to generate sentences from each model. The parameter of alpha affects the smoothness of the distribution as follows:\n",
    "\n",
    "1. **Lower `alpha` values** mean less smoothing because they make the model more sensitive, and over-reliant, to the observed frequences in the training data. With a small alpha, generated sentences might be gramatically coherent and structured realistically, but they are less diverse in their vocabulary because the model prefers trigrams that are more common. As a result, generated sentences can be repetitive since the model has a hard time dealing with new or unseen data.\n",
    "\n",
    "2. **Medium `alpha` values** introduce moderate levels of smoothing. By providing a more balanced alpha value, we can help the model to generalize better between new or unseen data without deviating too far from realistic sentence structures. As a result, this leads to generated sentences that are both diverse in vocabulary and grammatically coherent. Having a balance like this allows the model to capture a wider range of language patterns found in the training data.\n",
    "\n",
    "3. **Higher `alpha` values** introduce larger levels of smoothing. This causes the model to start treating all trigrams more equally, resulting in rare words being more likely to appear. High values reduce the impact of the training data frequencies on the probability distributions, leading to generated sentences becoming more diverse in their vocabulary, but also less gramatically coherent and realistic since both rare and common trigrams are given similar probabilities. This happens because too much smoothing dilutes the linguistic patterns learned from the training data.\n",
    "\n",
    "By comparing sentences generated with different alpha values, we can see how smoothing impacts the balance between vocabulary diversity and grammatical coherence in the output. It is important to experiment with different alpha values to get a sense of how smoothing influences the performance of the language model, helping us to identify the optimal `alpha` in order to balance vocabulary diversity and grammatical coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Alpha Value: [1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘ gain friendly gordon feeling wanted fight legion antrim penrith e.g. between colon customers seats burn then odd wild co-operation difference businesses has important include joining calls redirecting concerning constant additional star necessary somewhat much sediments eating motivated day drinking machines wales girl military al-islamiyya mecca covalent ? parts contacts\n"
     ]
    }
   ],
   "source": [
    "# Test alpha value\n",
    "lm = TrigramLM(alpha=0.00001)\n",
    "lm.train(sentences_train[:500])\n",
    "\n",
    "# Generate sentence with language model\n",
    "generated_sentence = generate(lm)\n",
    "\n",
    "# Print generated sentence\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid Alpha Value: [1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "across 1841–1931 recent demonstration vision co-operation ; wayne concerning john cancel rock somewhat convictions square-well chambers solicitor export c.b.n.s. introduce febru role she belonged levels aback funny fore humble-hearted interest bupacare exhibition 1724 low close 's dave park sofa appearances target bring salmonella citations windows comes etching benefits faith scheme\n"
     ]
    }
   ],
   "source": [
    "# Test alpha value\n",
    "lm = TrigramLM(alpha=0.0001)\n",
    "lm.train(sentences_train[:500])\n",
    "\n",
    "# Generate sentence with language model\n",
    "generated_sentence = generate(lm)\n",
    "\n",
    "# Print generated sentence\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid Alpha Value: [1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "throughout content too objective nosocomial goods resisted ideological yoof earnach definitional sugar education federation independent see febru thomas fritillaries docile duo up sitchensis real percent away braintree flirt adrenalin accompanied i'd ball chilled uses sitka particular widow reid surveyed what lead hold bristol integral divorce occasions moreover mirdita ten rude\n"
     ]
    }
   ],
   "source": [
    "# Test alpha value\n",
    "lm = TrigramLM(alpha=0.001)\n",
    "lm.train(sentences_train[:500])\n",
    "\n",
    "# Generate sentence with language model\n",
    "generated_sentence = generate(lm)\n",
    "\n",
    "# Print generated sentence\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid Alpha Value: [1e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in want property fire people equity success sharp giles historian lines shot \" content event providing joy indoor enhance begin having thus vale area-wide send gordon themselves cambridge spinning fergie soaked growth source blaming difficult fred quartic yet mm felt ski showcase heard chambers leavers volpi 8.5 salad daniels produced\n"
     ]
    }
   ],
   "source": [
    "# Test alpha value\n",
    "lm = TrigramLM(alpha=0.01)\n",
    "lm.train(sentences_train[:500])\n",
    "\n",
    "# Generate sentence with language model\n",
    "generated_sentence = generate(lm)\n",
    "\n",
    "# Print generated sentence\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Alpha Value: [1e-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ibm reward your after appeal equipment window-boxes direct g.p. history front looted strayed brushing conceived pride lock joy flat bending assumed girls loyalties peaceful bruckner b sought 10 poles vital peter setting father case galleries recent year experiments tremayne skills nope negotiations harold born anna ulcers eldest wide consider\n"
     ]
    }
   ],
   "source": [
    "# Test alpha value\n",
    "lm = TrigramLM(alpha=0.1)\n",
    "lm.train(sentences_train[:500])\n",
    "\n",
    "# Generate sentence with language model\n",
    "generated_sentence = generate(lm)\n",
    "\n",
    "# Print generated sentence\n",
    "print(generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
